{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mishnah_path = r'/Users/lee/Judaism/Sefaria_Fun/sefaria_txt/Mishnah/'\n",
    "gemara_path = r'/Users/lee/Judaism/Sefaria_Fun/sefaria_txt/Talmud/Bavli/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Gemara."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_mishnah = open(mishnah_path + 'Seder Zeraim/Mishnah Berakhot/Hebrew/merged.txt', 'rb').read().decode(encoding='utf-8')\n",
    "example_gemara = open(gemara_path + 'Seder Nezikin/Sanhedrin/English/merged.txt', 'rb').read().decode(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(example_gemara)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "headerless_gemara = example_gemara[example_gemara.find('\\n\\n\\n'):][2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dafless_gemara = re.sub(r'\\n\\nDaf .*\\n', '\\n', headerless_gemara)[2:]\n",
    "dafless_gemara = re.sub(r'\\n\\n\\n\\n', '', dafless_gemara)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(dafless_gemara)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_gemara = re.sub(r'<.*?>', '', dafless_gemara)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(clean_gemara)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove extra lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_gemara = re.sub('\\n\\n', '\\n', clean_gemara)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate through all files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = [x[0] for x in os.walk(gemara_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "english = [i for i in directories if 'English' in i and not 'Commentary' in i] \n",
    "hebrew = [i for i in directories if 'Hebrew' in i and not 'Commentary' in i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir in english:\n",
    "    gemara = open(dir + '/merged.txt', 'rb').read().decode(encoding='utf-8')\n",
    "    gemara = gemara[gemara.find('\\n\\n\\n\\n'):][2:]\n",
    "    gemara = re.sub(r'\\n\\nDaf .*\\n', '\\n', gemara)[2:]\n",
    "    gemara = re.sub(r'\\n\\n\\n\\n', '', gemara)\n",
    "    gemara = re.sub(r'<.*?>', '', gemara)\n",
    "    gemara = re.sub('\\n\\n', '\\n', gemara)\n",
    "    gemara += '\\n\\n'\n",
    "    \n",
    "    text_file = open(dir + '/clean.txt', 'wt')\n",
    "    n = text_file.write(gemara)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir in hebrew:\n",
    "    gemara = open(dir + '/merged.txt', 'rb').read().decode(encoding='utf-8')\n",
    "    gemara = gemara[gemara.find('\\n\\n\\n\\n'):][2:]\n",
    "    gemara = re.sub(r'\\n\\nDaf .*\\n', '\\n', gemara)[2:]\n",
    "    gemara = re.sub(r'\\n\\n\\n\\n', '', gemara)\n",
    "    gemara = re.sub(r'<.*?>', '', gemara)\n",
    "    gemara = re.sub('\\n\\n', '\\n', gemara)\n",
    "    gemara += '\\n\\n'\n",
    "    \n",
    "    text_file = open(dir + '/clean.txt', 'wt')\n",
    "    n = text_file.write(gemara)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = [x[0] for x in os.walk(gemara_path)]\n",
    "english = [i for i in directories if 'English' in i and not 'Commentary' in i] \n",
    "hebrew = [i for i in directories if 'Hebrew' in i and not 'Commentary' in i] \n",
    "\n",
    "gemara_english = ''\n",
    "gemara_hebrew = ''\n",
    "\n",
    "for direct in english:\n",
    "    gemara = open(direct + '/clean.txt', 'rb').read().decode(encoding='utf-8')\n",
    "    gemara_english += gemara\n",
    "\n",
    "for direct in hebrew:\n",
    "    gemara = open(direct + '/clean.txt', 'rb').read().decode(encoding='utf-8')\n",
    "    gemara_hebrew += gemara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"gemara_english.txt\", \"w\") as text_file:\n",
    "    text_file.write(gemara_english)\n",
    "\n",
    "with open(\"gemara_hebramaic.txt\", \"w\") as text_file:\n",
    "    text_file.write(gemara_hebrew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and prepare for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TalmudTokenizer:\n",
    "    def __init__(self, vocab_size: int = 16000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab: Dict[str, int] = {\"<PAD>\": 0, \"<UNK>\": 1, \"<BOS>\": 2, \"<EOS>\": 3}\n",
    "        self.inverse_vocab: Dict[int, str] = {v: k for k, v in self.vocab.items()}\n",
    "        self.merges: Dict[Tuple[str, str], str] = {}\n",
    "        self.space_prefix = 'Ä '\n",
    "\n",
    "    def _get_stats(self, vocab):\n",
    "        pairs = defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[symbols[i], symbols[i + 1]] += freq\n",
    "        return pairs\n",
    "\n",
    "    def _merge_vocab(self, pair, v_in):\n",
    "        v_out = {}\n",
    "        bigram = re.escape(' '.join(pair))\n",
    "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "        for word in v_in:\n",
    "            w_out = p.sub(''.join(pair), word)\n",
    "            v_out[w_out] = v_in[word]\n",
    "        return v_out\n",
    "\n",
    "    def train(self, text: str):\n",
    "        print(\"Starting tokenizer training...\")\n",
    "        \n",
    "        # Preprocess text to add space prefix, including for the first word\n",
    "        words = [self.space_prefix + word for word in text.split()]\n",
    "        \n",
    "        # Also add non-prefixed versions of words to the vocabulary\n",
    "        non_prefixed_words = text.split()\n",
    "        \n",
    "        # Initialize vocab with character tokens\n",
    "        chars = set(''.join(words + non_prefixed_words))\n",
    "        for char in chars:\n",
    "            if char not in self.vocab:\n",
    "                self.vocab[char] = len(self.vocab)\n",
    "                self.inverse_vocab[len(self.vocab) - 1] = char\n",
    "\n",
    "        print(f\"Initial vocabulary size: {len(self.vocab)}\")\n",
    "        \n",
    "        # Convert words to space-separated character sequences\n",
    "        vocab = Counter(' '.join(word) for word in words)\n",
    "        vocab.update(' '.join(word) for word in non_prefixed_words)\n",
    "        \n",
    "        num_merges = self.vocab_size - len(self.vocab)\n",
    "        for i in range(num_merges):\n",
    "            pairs = self._get_stats(vocab)\n",
    "            if not pairs:\n",
    "                print(f\"No more pairs to merge after {i} iterations\")\n",
    "                break\n",
    "            \n",
    "            best = max(pairs, key=pairs.get)\n",
    "            vocab = self._merge_vocab(best, vocab)\n",
    "            self.merges[best] = ''.join(best)\n",
    "            new_token = ''.join(best)\n",
    "            \n",
    "            if new_token not in self.vocab:\n",
    "                self.vocab[new_token] = len(self.vocab)\n",
    "                self.inverse_vocab[len(self.vocab) - 1] = new_token\n",
    "            \n",
    "            if len(self.vocab) >= self.vocab_size:\n",
    "                print(f\"Reached target vocabulary size after {i+1} iterations\")\n",
    "                break\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(f\"Completed {i} merges. Current vocab size: {len(self.vocab)}\")\n",
    "\n",
    "        print(f\"Final vocabulary size: {len(self.vocab)}\")\n",
    "        print(f\"Number of merges: {len(self.merges)}\")\n",
    "\n",
    "    def _tokenize_word(self, word: str) -> List[str]:\n",
    "        if word in self.vocab:\n",
    "            return [word]\n",
    "        \n",
    "        word = ' '.join(word)\n",
    "        tokens = []\n",
    "        while len(word) > 0:\n",
    "            subword = word\n",
    "            while len(subword) > 0:\n",
    "                if subword in self.vocab:\n",
    "                    tokens.append(subword)\n",
    "                    word = word[len(subword):].lstrip()\n",
    "                    break\n",
    "                subword = subword[:-1]\n",
    "            if len(subword) == 0:\n",
    "                tokens.append(word[0])\n",
    "                word = word[1:].lstrip()\n",
    "        return tokens\n",
    "\n",
    "    def tokenize(self, text: str) -> List[int]:\n",
    "        words = text.split()\n",
    "        tokens = []\n",
    "        for i, word in enumerate(words):\n",
    "            if i == 0 or word.startswith(self.space_prefix):\n",
    "                tokens.extend(self._tokenize_word(word))\n",
    "            else:\n",
    "                tokens.extend(self._tokenize_word(self.space_prefix + word))\n",
    "        return [self.vocab.get(token, self.vocab[\"<UNK>\"]) for token in tokens]\n",
    "\n",
    "    def decode(self, token_ids: List[int]) -> str:\n",
    "        tokens = [self.inverse_vocab.get(id, \"<UNK>\") for id in token_ids]\n",
    "        text = ''.join(tokens).replace(self.space_prefix, ' ')\n",
    "        return text.strip()\n",
    "\n",
    "    def save(self, path: str):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        with open(os.path.join(path, 'vocab.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.vocab, f, ensure_ascii=False, indent=2)\n",
    "        with open(os.path.join(path, 'merges.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump({' '.join(k): v for k, v in self.merges.items()}, f, ensure_ascii=False, indent=2)\n",
    "        with open(os.path.join(path, 'config.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump({'vocab_size': self.vocab_size, 'space_prefix': self.space_prefix}, f, indent=2)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str):\n",
    "        with open(os.path.join(path, 'config.json'), 'r', encoding='utf-8') as f:\n",
    "            config = json.load(f)\n",
    "        tokenizer = cls(vocab_size=config['vocab_size'])\n",
    "        tokenizer.space_prefix = config['space_prefix']\n",
    "        \n",
    "        with open(os.path.join(path, 'vocab.json'), 'r', encoding='utf-8') as f:\n",
    "            tokenizer.vocab = json.load(f)\n",
    "        tokenizer.inverse_vocab = {int(v): k for k, v in tokenizer.vocab.items()}\n",
    "        \n",
    "        with open(os.path.join(path, 'merges.json'), 'r', encoding='utf-8') as f:\n",
    "            merges = json.load(f)\n",
    "            tokenizer.merges = {tuple(k.split()): v for k, v in merges.items()}\n",
    "        \n",
    "        return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and save the tokenizer.\n",
    "tokenizer = TalmudTokenizer(vocab_size=16000)\n",
    "tokenizer.train(gemara)\n",
    "tokenizer.save(\"talmud_tokenizer\")\n",
    "print(\"Tokenizer saved.\")\n",
    "\n",
    "# Load the saved tokenizer\n",
    "loaded_tokenizer = TalmudTokenizer.load(\"talmud_tokenizer\")\n",
    "print(\"Tokenizer loaded.\")\n",
    "\n",
    "# Test the loaded tokenizer\n",
    "test_sentence = \"Rav Pappa said to Rabbi Akiva, from where do we learn about sandwiches?\"\n",
    "encoded = loaded_tokenizer.tokenize(test_sentence)\n",
    "decoded = loaded_tokenizer.decode(encoded)\n",
    "\n",
    "print(f\"\\nTest sentence: {test_sentence}\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")\n",
    "\n",
    "# Verify that the loaded tokenizer produces the same results as the original\n",
    "original_encoded = loaded_tokenizer.tokenize(test_sentence)\n",
    "print(f\"\\nOriginal encoded: {original_encoded}\")\n",
    "print(f\"Loaded tokenizer encoded: {encoded}\")\n",
    "print(f\"Encodings match: {original_encoded == encoded}\")\n",
    "\n",
    "# Print some statistics\n",
    "print(f\"\\nVocabulary size: {len(loaded_tokenizer.vocab)}\")\n",
    "print(f\"Number of merges: {len(loaded_tokenizer.merges)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, tokens, sequence_length):\n",
    "        self.tokens = tokens\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.tokens[idx:idx + self.sequence_length + 1]\n",
    "        return t.tensor(chunk[:-1], dtype=t.long), t.tensor(chunk[1:], dtype=t.long)\n",
    "\n",
    "def prepare_data_for_training(tokens, sequence_length, batch_size, val_split=0.1):\n",
    "    dataset = SequenceDataset(tokens, sequence_length)\n",
    "    \n",
    "    # Split into train and validation sets\n",
    "    val_size = int(val_split * len(dataset))\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
