{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3572a1e0-7af3-49ee-becc-bedbd97a1a21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3572a1e0-7af3-49ee-becc-bedbd97a1a21",
    "outputId": "7ef920cf-3e69-46cb-b9b3-d325dd76c6c8"
   },
   "outputs": [],
   "source": [
    "# Set up notebook.\n",
    "try:\n",
    "    import google.colab \n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Install packages\n",
    "    %pip install transformer_lens\n",
    "    %pip install einops\n",
    "    %pip install jaxtyping\n",
    "    import os, sys\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d537797b-50df-42b1-a1fb-66d62008c244",
   "metadata": {
    "id": "d537797b-50df-42b1-a1fb-66d62008c244"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import os; os.environ['ACCELERATE_DISABLE_RICH'] = \"1\"\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "from functools import partial\n",
    "from typing import Tuple, List, Optional, Dict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from jaxtyping import Float, Int\n",
    "from rich.table import Table\n",
    "from rich import print as rprint\n",
    "from pathlib import Path\n",
    "\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, random_split\n",
    "import einops\n",
    "\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "MAIN = __name__ == '__main__'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b87fefc-4142-447a-a607-8f481b6b3103",
   "metadata": {
    "id": "8b87fefc-4142-447a-a607-8f481b6b3103"
   },
   "outputs": [],
   "source": [
    "# Define GELU approximation function.\n",
    "SQRT = np.sqrt(2/np.pi)\n",
    "def geluprox(x):\n",
    "    return t.tanh(SQRT*(x + 0.044715*x**3))\n",
    "\n",
    "# Define softmax.\n",
    "def softmax(z):\n",
    "    return t.exp(z)/t.sum(t.exp(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "428f7564-016e-46a3-8624-3aeabbdfd853",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "428f7564-016e-46a3-8624-3aeabbdfd853",
    "outputId": "e989644a-e442-4052-8476-1435b74bad00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 unique Hebrew characters\n"
     ]
    }
   ],
   "source": [
    "# Load the data.\n",
    "gemara = open('./gemara_english.txt', 'rb').read().decode(encoding='utf-8')\n",
    "vocab  = sorted(set(gemara))\n",
    "print ('{} unique Hebrew characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6fe122-c8e9-401f-a252-f0c56e30f2cc",
   "metadata": {
    "id": "ab6fe122-c8e9-401f-a252-f0c56e30f2cc"
   },
   "source": [
    "## Build transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f247ad38-afc0-4bf9-8760-e1d2e40e2d3d",
   "metadata": {
    "id": "f247ad38-afc0-4bf9-8760-e1d2e40e2d3d"
   },
   "source": [
    "### Define architecture in config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "064bd2a5-0661-4ec1-9b99-a0f244321710",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "064bd2a5-0661-4ec1-9b99-a0f244321710",
    "outputId": "95e07994-d76a-4514-c40f-76443d7ba27a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(d_model=768, debug=True, layer_norm_eps=1e-05, d_vocab=16000, init_range=0.02, n_ctx=1024, d_head=64, d_mlp=3072, n_heads=12, n_layers=12)\n"
     ]
    }
   ],
   "source": [
    "dm = 128\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int = 768\n",
    "    debug: bool = True\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    d_vocab: int = 16000\n",
    "    init_range: float = 0.02\n",
    "    n_ctx: int = 1024\n",
    "    d_head: int = 64\n",
    "    d_mlp: int = 3072\n",
    "    n_heads: int = 12\n",
    "    n_layers: int = 12\n",
    "\n",
    "cfg = Config()\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "XytGgEcaWmFC",
   "metadata": {
    "id": "XytGgEcaWmFC"
   },
   "outputs": [],
   "source": [
    "def rand_float_test(cls, shape):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).to(device)\n",
    "    random_input = t.randn(shape).to(device)\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = layer(random_input)\n",
    "    if isinstance(output, tuple): output = output[0]\n",
    "    print(\"Output shape:\", output.shape, \"\\n\")\n",
    "\n",
    "def rand_int_test(cls, shape):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).to(device)\n",
    "    random_input = t.randint(100, 1000, shape).to(device)\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = layer(random_input)\n",
    "    if isinstance(output, tuple): output = output[0]\n",
    "    print(\"Output shape:\", output.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9eb3fe-77e7-42a6-a05d-0b8a7e263182",
   "metadata": {
    "id": "9d9eb3fe-77e7-42a6-a05d-0b8a7e263182"
   },
   "source": [
    "### Define Layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "071a5eff-b8cc-48bc-a0c6-4d0ae8093283",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "071a5eff-b8cc-48bc-a0c6-4d0ae8093283",
    "outputId": "0493715a-a215-4524-8904-7fa9fa673a0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Embed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_E = nn.Parameter(t.empty((cfg.d_vocab, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        return self.W_E[tokens]\n",
    "\n",
    "rand_int_test(Embed, [2, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32734560-e44e-497e-b5d5-463cdc280965",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "32734560-e44e-497e-b5d5-463cdc280965",
    "outputId": "c12ff520-9839-4c56-d660-7800ad0a3ebf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.w = nn.Parameter(t.ones(cfg.d_model))\n",
    "        self.b = nn.Parameter(t.zeros(cfg.d_model))\n",
    "\n",
    "    def forward(self, residual):\n",
    "        mean = residual.mean(dim=-1, keepdim=True)\n",
    "        vary = (residual.var(dim=-1, keepdim=True, unbiased=False) +\n",
    "                            self.cfg.layer_norm_eps).sqrt()\n",
    "        residual = (residual - mean)/vary\n",
    "        return residual * self.w + self.b\n",
    "\n",
    "rand_float_test(LayerNorm, [2, 4, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f73c264-8ac3-4ba5-b308-dc36869f4ba0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4f73c264-8ac3-4ba5-b308-dc36869f4ba0",
    "outputId": "8a34f473-5009-4788-cbc9-7bccc25411e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_pos = nn.Parameter(t.empty((cfg.n_ctx, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        #batch, seq_len = tokens.shape\n",
    "        shapey = tokens.shape\n",
    "        batch = shapey[0]\n",
    "        seq_len = shapey[1]\n",
    "        #return einops.repeat(self.W_pos[:seq_len], \"seq d_model -> batch seq d_model\", batch=batch)\n",
    "        return self.W_pos[:seq_len].unsqueeze(0).expand(batch, -1, -1)\n",
    "\n",
    "rand_int_test(PosEmbed, [2, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f21c931-0360-4b28-b830-e66b226cfa3e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2f21c931-0360-4b28-b830-e66b226cfa3e",
    "outputId": "b05a82c8-b948-4000-f289-561236816b28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    IGNORE: Float[Tensor, \"\"]\n",
    "\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_Q = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.W_K = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.W_V = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.W_O = nn.Parameter(t.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
    "        self.b_Q = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_K = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_V = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_O = nn.Parameter(t.zeros((cfg.d_model)))\n",
    "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
    "        self.register_buffer(\"IGNORE\", t.tensor(-1e5, dtype=t.float32, device=device))\n",
    "\n",
    "    def forward(\n",
    "        self, x: Float[Tensor, \"batch posn d_model\"]\n",
    "    ) -> Float[Tensor, \"batch posn d_model\"]: #normalized_resid_pre\n",
    "        # x: residual stream, (batch, seq_len, d_model)\n",
    "        # Compute key, query, and value tensors.\n",
    "        # n heads, b batches, sequence length s, embedding e (d_model), head size h.\n",
    "        K = t.einsum('nmh,bsm->bsnh', self.W_K, x) + self.b_K # (b, s, n, h)\n",
    "        Q = t.einsum('nmh,bsm->bsnh', self.W_Q, x) + self.b_Q # (b, s, n, h)\n",
    "        V = t.einsum('nmh,bsm->bsnh', self.W_V, x) + self.b_V # (b, s, n, h)\n",
    "        # Compute attention scores.\n",
    "        QKt = t.einsum('bsnh,btnh->bnst', Q, K) # (b, n, s, s)\n",
    "        # Compute attention probabilities, with causal mask.\n",
    "        A = self.apply_causal_mask(QKt/self.cfg.d_head**0.5).softmax(-1) # (b, n, s, s)\n",
    "        # Sum over value vectors, weighted by A.\n",
    "        z = t.einsum(\"bnst,btnh->bsnh\", A, V) # (b, s, n, h)\n",
    "        # Sum over parallel attention heads.\n",
    "        attention = t.einsum(\"nhm,bsnh->bsm\", self.W_O, z)\n",
    "        return attention\n",
    "\n",
    "    def apply_causal_mask(\n",
    "        self, attn_scores: Float[Tensor, \"batch n_heads query_pos key_pos\"]\n",
    "    ) -> Float[Tensor, \"batch n_heads query_pos key_pos\"]:\n",
    "        '''\n",
    "        Applies a causal mask to attention scores, and returns masked scores.\n",
    "        '''\n",
    "        onesy = t.ones(attn_scores.size(-2), attn_scores.size(-1), device=attn_scores.device)\n",
    "        mask = t.triu(onesy, diagonal=1).bool()\n",
    "        # Apply the mask to attention scores, then return the masked scores\n",
    "        attn_scores.masked_fill_(mask, self.IGNORE)\n",
    "        return attn_scores\n",
    "\n",
    "rand_float_test(Attention, [2, 4, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "839e5dd7-ab9c-485a-be48-023ae6f985f1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "839e5dd7-ab9c-485a-be48-023ae6f985f1",
    "outputId": "cf204873-11bd-448a-ed4e-debe509c64a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_in = nn.Parameter(t.empty((cfg.d_model, cfg.d_mlp)))\n",
    "        self.W_out = nn.Parameter(t.empty((cfg.d_mlp, cfg.d_model)))\n",
    "        self.b_in = nn.Parameter(t.zeros((cfg.d_mlp)))\n",
    "        self.b_out = nn.Parameter(t.zeros((cfg.d_model)))\n",
    "        nn.init.normal_(self.W_in, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(\n",
    "        self, x: Float[Tensor, \"batch posn d_model\"]\n",
    "    ) -> Float[Tensor, \"batch posn d_model\"]:\n",
    "        neural = t.einsum('mn,bsm->bsn', self.W_in, x) + self.b_in\n",
    "        neural = geluprox(neural)\n",
    "        return t.einsum('nm,bsn->bsm', self.W_out, neural) + self.b_out\n",
    "\n",
    "rand_float_test(MLP, [2, 4, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa55eb62-bcdf-4b72-966e-5103d935b1f0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa55eb62-bcdf-4b72-966e-5103d935b1f0",
    "outputId": "16e8896a-7ffa-4cea-c204-3ca6097ec468"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.ln1 = LayerNorm(cfg)\n",
    "        self.attn = Attention(cfg)\n",
    "        self.ln2 = LayerNorm(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "\n",
    "    def forward(\n",
    "        self, x: Float[Tensor, \"batch position d_model\"]\n",
    "    ) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        y = self.attn(self.ln1(x)) + x\n",
    "        z = self.mlp(self.ln2(y)) + y\n",
    "        return z\n",
    "\n",
    "rand_float_test(TransformerBlock, [2, 4, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e736265f-83bd-468b-9d56-c41f97302f39",
   "metadata": {
    "id": "e736265f-83bd-468b-9d56-c41f97302f39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 16000]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Unembed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_U = nn.Parameter(t.empty((cfg.d_model, cfg.d_vocab)))\n",
    "        nn.init.normal_(self.W_U, std=self.cfg.init_range)\n",
    "        self.b_U = nn.Parameter(t.zeros((cfg.d_vocab), requires_grad=False))\n",
    "\n",
    "    def forward(\n",
    "        self, x: Float[Tensor, \"batch position d_model\"]\n",
    "    ) -> Float[Tensor, \"batch position d_vocab\"]:\n",
    "        return t.einsum('mv,bsm->bsv', self.W_U, x) + self.b_U\n",
    "\n",
    "rand_float_test(Unembed, [2, 4, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da586ad0-3710-4f72-a587-0e51ec6b1ac0",
   "metadata": {
    "id": "da586ad0-3710-4f72-a587-0e51ec6b1ac0"
   },
   "outputs": [],
   "source": [
    "class RoboRav(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg       = cfg\n",
    "        self.embed     = Embed(cfg)\n",
    "        self.pos_embed = PosEmbed(cfg)\n",
    "        self.blocks    = nn.ModuleList([TransformerBlock(cfg) for\n",
    "                                        _ in range(cfg.n_layers)])\n",
    "        self.ln_final  = LayerNorm(cfg)\n",
    "        self.unembed   = Unembed(cfg)\n",
    "\n",
    "    def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_vocab\"]:\n",
    "        residual = self.embed(tokens) + self.pos_embed(tokens)\n",
    "        for block in self.blocks:\n",
    "            residual = block(residual)\n",
    "        logits = self.unembed(self.ln_final(residual))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9bd2eb-30f4-4658-9a55-a21ae652c994",
   "metadata": {
    "id": "aa9bd2eb-30f4-4658-9a55-a21ae652c994"
   },
   "source": [
    "## Train on Gemara. Start by tokenizing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f66259c3-140b-47c7-8a1a-cddaa1b86eb6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wJZpbcuirW0K",
    "outputId": "fd2214dc-32c5-4254-9831-f0856469ae17"
   },
   "outputs": [],
   "source": [
    "class TalmudTokenizer:\n",
    "    def __init__(self, vocab_size: int = 16000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab: Dict[str, int] = {\"<PAD>\": 0, \"<UNK>\": 1, \"<BOS>\": 2, \"<EOS>\": 3}\n",
    "        self.inverse_vocab: Dict[int, str] = {v: k for k, v in self.vocab.items()}\n",
    "        self.merges: Dict[Tuple[str, str], str] = {}\n",
    "        self.space_prefix = 'Ä '\n",
    "\n",
    "    def _get_stats(self, vocab):\n",
    "        pairs = defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[symbols[i], symbols[i + 1]] += freq\n",
    "        return pairs\n",
    "\n",
    "    def _merge_vocab(self, pair, v_in):\n",
    "        v_out = {}\n",
    "        bigram = re.escape(' '.join(pair))\n",
    "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "        for word in v_in:\n",
    "            w_out = p.sub(''.join(pair), word)\n",
    "            v_out[w_out] = v_in[word]\n",
    "        return v_out\n",
    "\n",
    "    def train(self, text: str):\n",
    "        print(\"Starting tokenizer training...\")\n",
    "        \n",
    "        # Preprocess text to add space prefix, including for the first word\n",
    "        words = [self.space_prefix + word for word in text.split()]\n",
    "        \n",
    "        # Also add non-prefixed versions of words to the vocabulary\n",
    "        non_prefixed_words = text.split()\n",
    "        \n",
    "        # Initialize vocab with character tokens\n",
    "        chars = set(''.join(words + non_prefixed_words))\n",
    "        for char in chars:\n",
    "            if char not in self.vocab:\n",
    "                self.vocab[char] = len(self.vocab)\n",
    "                self.inverse_vocab[len(self.vocab) - 1] = char\n",
    "\n",
    "        print(f\"Initial vocabulary size: {len(self.vocab)}\")\n",
    "        \n",
    "        # Convert words to space-separated character sequences\n",
    "        vocab = Counter(' '.join(word) for word in words)\n",
    "        vocab.update(' '.join(word) for word in non_prefixed_words)\n",
    "        \n",
    "        num_merges = self.vocab_size - len(self.vocab)\n",
    "        for i in range(num_merges):\n",
    "            pairs = self._get_stats(vocab)\n",
    "            if not pairs:\n",
    "                print(f\"No more pairs to merge after {i} iterations\")\n",
    "                break\n",
    "            \n",
    "            best = max(pairs, key=pairs.get)\n",
    "            vocab = self._merge_vocab(best, vocab)\n",
    "            self.merges[best] = ''.join(best)\n",
    "            new_token = ''.join(best)\n",
    "            \n",
    "            if new_token not in self.vocab:\n",
    "                self.vocab[new_token] = len(self.vocab)\n",
    "                self.inverse_vocab[len(self.vocab) - 1] = new_token\n",
    "            \n",
    "            if len(self.vocab) >= self.vocab_size:\n",
    "                print(f\"Reached target vocabulary size after {i+1} iterations\")\n",
    "                break\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(f\"Completed {i} merges. Current vocab size: {len(self.vocab)}\")\n",
    "\n",
    "        print(f\"Final vocabulary size: {len(self.vocab)}\")\n",
    "        print(f\"Number of merges: {len(self.merges)}\")\n",
    "\n",
    "    def _tokenize_word(self, word: str) -> List[str]:\n",
    "        if word in self.vocab:\n",
    "            return [word]\n",
    "        \n",
    "        word = ' '.join(word)\n",
    "        tokens = []\n",
    "        while len(word) > 0:\n",
    "            subword = word\n",
    "            while len(subword) > 0:\n",
    "                if subword in self.vocab:\n",
    "                    tokens.append(subword)\n",
    "                    word = word[len(subword):].lstrip()\n",
    "                    break\n",
    "                subword = subword[:-1]\n",
    "            if len(subword) == 0:\n",
    "                tokens.append(word[0])\n",
    "                word = word[1:].lstrip()\n",
    "        return tokens\n",
    "\n",
    "    def tokenize(self, text: str) -> List[int]:\n",
    "        words = text.split()\n",
    "        tokens = []\n",
    "        for i, word in enumerate(words):\n",
    "            if i == 0 or word.startswith(self.space_prefix):\n",
    "                tokens.extend(self._tokenize_word(word))\n",
    "            else:\n",
    "                tokens.extend(self._tokenize_word(self.space_prefix + word))\n",
    "        return [self.vocab.get(token, self.vocab[\"<UNK>\"]) for token in tokens]\n",
    "\n",
    "    def decode(self, token_ids: List[int]) -> str:\n",
    "        tokens = [self.inverse_vocab.get(id, \"<UNK>\") for id in token_ids]\n",
    "        text = ''.join(tokens).replace(self.space_prefix, ' ')\n",
    "        return text.strip()\n",
    "\n",
    "    def save(self, path: str):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        with open(os.path.join(path, 'vocab.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.vocab, f, ensure_ascii=False, indent=2)\n",
    "        with open(os.path.join(path, 'merges.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump({' '.join(k): v for k, v in self.merges.items()}, f, ensure_ascii=False, indent=2)\n",
    "        with open(os.path.join(path, 'config.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump({'vocab_size': self.vocab_size, 'space_prefix': self.space_prefix}, f, indent=2)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str):\n",
    "        with open(os.path.join(path, 'config.json'), 'r', encoding='utf-8') as f:\n",
    "            config = json.load(f)\n",
    "        tokenizer = cls(vocab_size=config['vocab_size'])\n",
    "        tokenizer.space_prefix = config['space_prefix']\n",
    "        \n",
    "        with open(os.path.join(path, 'vocab.json'), 'r', encoding='utf-8') as f:\n",
    "            tokenizer.vocab = json.load(f)\n",
    "        tokenizer.inverse_vocab = {int(v): k for k, v in tokenizer.vocab.items()}\n",
    "        \n",
    "        with open(os.path.join(path, 'merges.json'), 'r', encoding='utf-8') as f:\n",
    "            merges = json.load(f)\n",
    "            tokenizer.merges = {tuple(k.split()): v for k, v in merges.items()}\n",
    "        \n",
    "        return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eca58dd4-ea5f-43ed-8d99-44f784b1217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, tokens, sequence_length):\n",
    "        self.tokens = tokens\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.tokens[idx:idx + self.sequence_length + 1]\n",
    "        return t.tensor(chunk[:-1], dtype=t.long), t.tensor(chunk[1:], dtype=t.long)\n",
    "\n",
    "def prepare_data_for_training(tokens, sequence_length, batch_size, val_split=0.1):\n",
    "    dataset = SequenceDataset(tokens, sequence_length)\n",
    "    \n",
    "    # Split into train and validation sets\n",
    "    val_size = int(val_split * len(dataset))\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95b22e4a-77d0-413d-8725-6dcbde9f1f4f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wJZpbcuirW0K",
    "outputId": "fd2214dc-32c5-4254-9831-f0856469ae17",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved.\n",
      "Tokenizer loaded.\n",
      "\n",
      "Test sentence: Rav Pappa said to Rabbi Akiva, from where do we learn about sandwiches?\n",
      "Encoded: [212, 2444, 286, 137, 260, 5519, 256, 470, 512, 571, 2264, 1163, 89, 75, 68, 98, 24, 15, 7, 83, 53, 86, 75, 74]\n",
      "Decoded: Rav Pappa said to Rabbi Akiva, from where do we learn about sandwiches?\n",
      "\n",
      "Original encoded: [212, 2444, 286, 137, 260, 5519, 256, 470, 512, 571, 2264, 1163, 89, 75, 68, 98, 24, 15, 7, 83, 53, 86, 75, 74]\n",
      "Loaded tokenizer encoded: [212, 2444, 286, 137, 260, 5519, 256, 470, 512, 571, 2264, 1163, 89, 75, 68, 98, 24, 15, 7, 83, 53, 86, 75, 74]\n",
      "Encodings match: True\n",
      "\n",
      "Vocabulary size: 16000\n",
      "Number of merges: 15899\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "talmud_text = gemara\n",
    "# Train and save the tokenizer\n",
    "\"\"\"\n",
    "tokenizer = TalmudTokenizer(vocab_size=16000)\n",
    "tokenizer.train(talmud_text)\n",
    "tokenizer.save(\"talmud_tokenizer\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Tokenizer saved.\")\n",
    "\n",
    "# Load the saved tokenizer\n",
    "loaded_tokenizer = TalmudTokenizer.load(\"talmud_tokenizer\")\n",
    "print(\"Tokenizer loaded.\")\n",
    "\n",
    "# Test the loaded tokenizer\n",
    "test_sentence = \"Rav Pappa said to Rabbi Akiva, from where do we learn about sandwiches?\"\n",
    "encoded = loaded_tokenizer.tokenize(test_sentence)\n",
    "decoded = loaded_tokenizer.decode(encoded)\n",
    "\n",
    "print(f\"\\nTest sentence: {test_sentence}\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")\n",
    "\n",
    "# Verify that the loaded tokenizer produces the same results as the original\n",
    "original_encoded = loaded_tokenizer.tokenize(test_sentence)\n",
    "print(f\"\\nOriginal encoded: {original_encoded}\")\n",
    "print(f\"Loaded tokenizer encoded: {encoded}\")\n",
    "print(f\"Encodings match: {original_encoded == encoded}\")\n",
    "\n",
    "# Print some statistics\n",
    "print(f\"\\nVocabulary size: {len(loaded_tokenizer.vocab)}\")\n",
    "print(f\"Number of merges: {len(loaded_tokenizer.merges)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ea4b3f0-4907-4d8a-b617-d8666e8aa0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "talmudtokens = loaded_tokenizer.tokenize(talmud_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6163aa9-c5af-448f-afbd-3feda994ed04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12687031\n"
     ]
    }
   ],
   "source": [
    "print(len(talmudtokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98b256a2-bcaf-46f5-a0aa-611ffe37fbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = prepare_data_for_training(talmudtokens, 256, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6813a1-f85e-48ea-a28d-f516c428dbae",
   "metadata": {
    "id": "ce6813a1-f85e-48ea-a28d-f516c428dbae"
   },
   "source": [
    "### Initialize model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ea388b0-b6be-451d-903e-b02f1753326b",
   "metadata": {
    "id": "7ea388b0-b6be-451d-903e-b02f1753326b"
   },
   "outputs": [],
   "source": [
    "dm = 128\n",
    "cli = Config(\n",
    "    debug=False,\n",
    "    d_model=dm,\n",
    "    n_heads=4,\n",
    "    d_head=8,\n",
    "    d_mlp=4*dm,\n",
    "    n_layers=2,\n",
    "    n_ctx=256,\n",
    "    d_vocab=16000\n",
    ")\n",
    "roborav = RoboRav(cli)\n",
    "loss_fn = nn.CrossEntropyLoss()  # Replace with your loss function\n",
    "optimizer = optim.Adam(roborav.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e0486f-80d9-472b-be5a-b37d627d1ae9",
   "metadata": {},
   "source": [
    "### Define training model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "12aca029-5b2c-4e26-8385-e0aac521a8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, loss_fn, optimizer, num_epochs, device):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_train_losses = []\n",
    "        \n",
    "        # Training loop\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Reshape outputs and targets\n",
    "            batch_size, seq_length, vocab_size = outputs.shape\n",
    "            outputs = outputs.view(-1, vocab_size)  # Reshape to [batch_size * seq_length, vocab_size]\n",
    "            targets = targets.view(-1)  # Reshape to [batch_size * seq_length]\n",
    "            \n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_train_losses.append(loss.item())\n",
    "        \n",
    "        avg_train_loss = np.mean(epoch_train_losses)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        epoch_val_losses = []\n",
    "        with t.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs, targets = batch\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                # Reshape outputs and targets\n",
    "                batch_size, seq_length, vocab_size = outputs.shape\n",
    "                outputs = outputs.view(-1, vocab_size)  # Reshape to [batch_size * seq_length, vocab_size]\n",
    "                targets = targets.view(-1)  # Reshape to [batch_size * seq_length]\n",
    "                \n",
    "                loss = loss_fn(outputs, targets)\n",
    "                epoch_val_losses.append(loss.item())\n",
    "        \n",
    "        avg_val_loss = np.mean(epoch_val_losses)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Save the model after each epoch\n",
    "        t.save(model.state_dict(), f\"model_epoch_{epoch+1}.pth\")\n",
    "    \n",
    "    # Plot and save the loss curve\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, num_epochs+1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('loss_curve.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Save the losses to a file\n",
    "    np.savez('losses.npz', train_losses=train_losses, val_losses=val_losses)\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8636b044-5423-4788-8f10-a288f1ec726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train(roborav, train_loader, test_loader, loss_fn, optimizer, 1, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02f175f-8463-400a-9b6e-c58844ecbdfb",
   "metadata": {},
   "source": [
    "### Sample from the trained transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59889e2-ba56-4da5-804f-dce8d6b479ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
